#!/usr/bin/env python3
"""
DenseNet121 Transfer Learning cho ph√¢n lo·∫°i 4 l·ªõp ALL-IDB4
=========================================================

Ki·∫øn tr√∫c: DenseNet121 pretrained + custom classification head
Classes: Benign, Early_Pre_B_ALL, Pre_B_ALL, Pro_B_ALL

Author: Cell Image-Based Disease Detection Team
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import DenseNet121
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Import unified configuration - robust path resolution
import sys
from pathlib import Path

def find_project_src():
    """Find the src directory containing config.py"""
    current_path = Path(__file__).resolve()
    while current_path != current_path.parent:
        config_path = current_path / "src" / "config.py"
        if config_path.exists():
            return current_path / "src"
        current_path = current_path.parent
    return None

project_src = find_project_src()
if project_src:
    sys.path.insert(0, str(project_src))

from config import config, get_model_config

SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Get model-specific configuration
model_config = get_model_config("densenet121_transfer")
MODEL_NAME = "DenseNet121_Transfer_4Class"

class Config:
    # Dataset
    BASE_PATH = str(Path(__file__).parent.parent)
    DATASET_PATH = config.dataset.full_dataset_path
    CLASS_NAMES = config.dataset.class_names
    NUM_CLASSES = config.dataset.num_classes
    CLASS_WEIGHTS = config.dataset.class_weights
    
    # Model
    MODEL_NAME = MODEL_NAME
    INPUT_SHAPE = model_config["input_size"]
    DROPOUT_RATE = model_config["dropout_rate"]
    L2_REG = 1e-4
    DENSE_UNITS = model_config["dense_units"]
    
    # Training
    BATCH_SIZE = model_config["batch_size"]
    EPOCHS = model_config["epochs"]
    INITIAL_LR = model_config["learning_rate"]
    FINE_TUNE_LR = config.training.learning_rate_finetune
    
    # Transfer learning params
    FREEZE_LAYERS = 200  # Freeze first 200 layers
    FREEZE_EPOCHS = model_config["freeze_epochs"]
    FINE_TUNE_EPOCHS = model_config["finetune_epochs"]

def create_densenet121_transfer_model(fine_tune=False):
    """T·∫°o DenseNet121 transfer learning model"""
    print(f"üèóÔ∏è X√¢y d·ª±ng DenseNet121 Transfer Learning model (fine_tune={fine_tune})...")
    
    # Load pre-trained DenseNet121
    base_model = DenseNet121(
        weights='imagenet',
        include_top=False,
        input_shape=Config.INPUT_SHAPE
    )
    
    # Freeze base model layers
    if not fine_tune:
        for layer in base_model.layers[:Config.FREEZE_LAYERS]:
            layer.trainable = False
        print(f"üîí Frozen first {Config.FREEZE_LAYERS} layers")
    else:
        # Fine-tuning: unfreeze some layers
        for layer in base_model.layers:
            layer.trainable = True
        print("üîì All layers unfrozen for fine-tuning")
    
    # Add custom classification head optimized for DenseNet
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        
        # Dense layers optimized for DenseNet
        layers.Dense(Config.DENSE_UNITS[0], activation='relu',
                    kernel_regularizer=keras.regularizers.l2(Config.L2_REG)),
        layers.BatchNormalization(),
        layers.Dropout(Config.DROPOUT_RATE),
        
        layers.Dense(Config.DENSE_UNITS[1], activation='relu',
                    kernel_regularizer=keras.regularizers.l2(Config.L2_REG)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        
        # Output layer
        layers.Dense(Config.NUM_CLASSES, activation='softmax')
    ])
    
    model._name = Config.MODEL_NAME
    return model

def create_data_generators():
    """T·∫°o data generators v·ªõi DenseNet preprocessing"""
    print("üîÑ T·∫°o data generators...")
    
    # Training augmentation - optimized for DenseNet
    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=25,
        width_shift_range=0.15,
        height_shift_range=0.15,
        shear_range=0.1,
        zoom_range=0.15,
        horizontal_flip=True,
        vertical_flip=True,
        brightness_range=[0.8, 1.2],
        fill_mode='nearest'
    )
    
    # Validation/test data
    val_test_datagen = ImageDataGenerator(rescale=1./255)
    
    # Create generators
    train_generator = train_datagen.flow_from_directory(
        os.path.join(Config.DATASET_PATH, "train"),
        target_size=Config.INPUT_SHAPE[:2],
        batch_size=Config.BATCH_SIZE,
        class_mode='sparse',
        shuffle=True,
        seed=SEED
    )
    
    validation_generator = val_test_datagen.flow_from_directory(
        os.path.join(Config.DATASET_PATH, "val"),
        target_size=Config.INPUT_SHAPE[:2],
        batch_size=Config.BATCH_SIZE,
        class_mode='sparse',
        shuffle=False
    )
    
    test_generator = val_test_datagen.flow_from_directory(
        os.path.join(Config.DATASET_PATH, "test"),
        target_size=Config.INPUT_SHAPE[:2],
        batch_size=Config.BATCH_SIZE,
        class_mode='sparse',
        shuffle=False
    )
    
    print(f"‚úÖ Train: {train_generator.samples}, Val: {validation_generator.samples}, Test: {test_generator.samples}")
    
    return train_generator, validation_generator, test_generator

def setup_callbacks(model_save_dir, phase="initial"):
    """Thi·∫øt l·∫≠p callbacks"""
    os.makedirs(model_save_dir, exist_ok=True)
    
    checkpoint_path = os.path.join(model_save_dir, f"best_{Config.MODEL_NAME}_{phase}.h5")
    
    callbacks_list = [
        callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        ),
        callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15 if phase == "initial" else 10,
            restore_best_weights=True,
            verbose=1
        ),
        callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.3,  # More aggressive LR reduction for DenseNet
            patience=7 if phase == "initial" else 5,
            min_lr=1e-8,
            verbose=1
        ),
        # Cosine annealing for better convergence
        callbacks.LearningRateScheduler(
            lambda epoch, lr: lr * 0.95 if epoch > 10 else lr,
            verbose=0
        )
    ]
    
    return callbacks_list

def train_model_two_phase(model, train_gen, val_gen, model_save_dir):
    """Two-phase training: freeze then fine-tune"""
    
    # Phase 1: Frozen base model
    print("\nüöÄ PHASE 1: Training v·ªõi frozen DenseNet121 base")
    print("=" * 50)
    
    # Compile with custom optimizer settings for DenseNet
    model.compile(
        optimizer=optimizers.Adam(
            learning_rate=Config.INITIAL_LR,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        ),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    callbacks_list = setup_callbacks(model_save_dir, "phase1")
    
    steps_per_epoch = train_gen.samples // Config.BATCH_SIZE
    validation_steps = val_gen.samples // Config.BATCH_SIZE
    
    history1 = model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        epochs=Config.FREEZE_EPOCHS,
        validation_data=val_gen,
        validation_steps=validation_steps,
        callbacks=callbacks_list,
        class_weight=Config.CLASS_WEIGHTS,
        verbose=1
    )
    
    # Phase 2: Fine-tuning
    print("\nüî• PHASE 2: Fine-tuning to√†n b·ªô DenseNet121")
    print("=" * 50)
    
    # Recreate model for fine-tuning
    model = create_densenet121_transfer_model(fine_tune=True)
    
    # Load weights from phase 1
    phase1_model_path = os.path.join(model_save_dir, f"best_{Config.MODEL_NAME}_phase1.h5")
    if os.path.exists(phase1_model_path):
        print("üìÇ Loading phase 1 weights...")
        try:
            phase1_model = keras.models.load_model(phase1_model_path)
            model.set_weights(phase1_model.get_weights())
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load phase 1 weights: {e}")
    
    # Compile with lower learning rate for fine-tuning
    model.compile(
        optimizer=optimizers.Adam(
            learning_rate=Config.FINE_TUNE_LR,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-8
        ),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    callbacks_list = setup_callbacks(model_save_dir, "phase2")
    
    history2 = model.fit(
        train_gen,
        steps_per_epoch=steps_per_epoch,
        epochs=Config.FINE_TUNE_EPOCHS,
        validation_data=val_gen,
        validation_steps=validation_steps,
        callbacks=callbacks_list,
        class_weight=Config.CLASS_WEIGHTS,
        verbose=1
    )
    
    # Combine histories
    combined_history = {}
    for key in history1.history.keys():
        combined_history[key] = history1.history[key] + history2.history[key]
    
    return model, combined_history

def evaluate_model(model, test_generator, results_dir):
    """ƒê√°nh gi√° model v·ªõi detailed analysis"""
    print("üìä ƒê√°nh gi√° model...")
    
    os.makedirs(results_dir, exist_ok=True)
    
    # Predict
    test_generator.reset()
    predictions = model.predict(test_generator, verbose=1)
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = test_generator.classes
    
    # Classification report
    report = classification_report(
        true_classes, 
        predicted_classes, 
        target_names=Config.CLASS_NAMES,
        output_dict=True
    )
    
    print("\nüìà CLASSIFICATION REPORT:")
    print("=" * 60)
    print(classification_report(true_classes, predicted_classes, target_names=Config.CLASS_NAMES))
    
    # Confusion matrix
    cm = confusion_matrix(true_classes, predicted_classes)
    
    # Enhanced confusion matrix plot
    plt.figure(figsize=(12, 9))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=Config.CLASS_NAMES,
                yticklabels=Config.CLASS_NAMES,
                cbar_kws={'label': 'Count'})
    plt.title(f'{Config.MODEL_NAME} - Confusion Matrix\nOverall Accuracy: {report["accuracy"]:.4f}', 
              fontsize=14, fontweight='bold')
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    
    cm_path = os.path.join(results_dir, f"{Config.MODEL_NAME}_confusion_matrix.png")
    plt.savefig(cm_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Additional analysis: prediction confidence
    prediction_confidence = np.max(predictions, axis=1)
    confidence_by_class = {}
    for i, class_name in enumerate(Config.CLASS_NAMES):
        class_indices = np.where(true_classes == i)[0]
        if len(class_indices) > 0:
            confidence_by_class[class_name] = {
                'mean_confidence': np.mean(prediction_confidence[class_indices]),
                'std_confidence': np.std(prediction_confidence[class_indices])
            }
    
    # Save results
    results = {
        'model_name': Config.MODEL_NAME,
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'test_accuracy': report['accuracy'],
        'prediction_confidence': confidence_by_class,
        'per_class_metrics': {
            Config.CLASS_NAMES[i]: {
                'precision': report[Config.CLASS_NAMES[i]]['precision'],
                'recall': report[Config.CLASS_NAMES[i]]['recall'],
                'f1-score': report[Config.CLASS_NAMES[i]]['f1-score']
            } for i in range(Config.NUM_CLASSES)
        }
    }
    
    results_path = os.path.join(results_dir, f"{Config.MODEL_NAME}_results.json")
    # Convert numpy types to Python types for JSON serialization
    def convert_numpy_types(obj):
        if hasattr(obj, 'item'):
            return obj.item()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(v) for v in obj]
        return obj
    
    results = convert_numpy_types(results)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Results saved to: {results_path}")
    return results

def plot_training_history(history, results_dir):
    """V·∫Ω enhanced training history"""
    if history is None:
        return
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Add vertical line to separate phases
    phase_separator = Config.FREEZE_EPOCHS
    
    # Accuracy
    axes[0, 0].plot(history['accuracy'], label='Training', linewidth=2)
    axes[0, 0].plot(history['val_accuracy'], label='Validation', linewidth=2)
    axes[0, 0].axvline(x=phase_separator, color='red', linestyle='--', alpha=0.7, label='Fine-tune start')
    axes[0, 0].set_title('Model Accuracy', fontweight='bold')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Accuracy')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Loss
    axes[0, 1].plot(history['loss'], label='Training', linewidth=2)
    axes[0, 1].plot(history['val_loss'], label='Validation', linewidth=2)
    axes[0, 1].axvline(x=phase_separator, color='red', linestyle='--', alpha=0.7, label='Fine-tune start')
    axes[0, 1].set_title('Model Loss', fontweight='bold')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Precision (disabled)
    axes[0, 2].text(0.5, 0.5, 'Precision metric removed\nto avoid shape mismatch', 
                    horizontalalignment='center', verticalalignment='center',
                    transform=axes[0, 2].transAxes, fontsize=12)
    axes[0, 2].set_title('Precision (Disabled)', fontweight='bold')
    axes[0, 2].grid(True, alpha=0.3)
    
    # Recall (disabled)
    axes[1, 0].text(0.5, 0.5, 'Recall metric removed\nto avoid shape mismatch', 
                    horizontalalignment='center', verticalalignment='center',
                    transform=axes[1, 0].transAxes, fontsize=12)
    axes[1, 0].set_title('Recall (Disabled)', fontweight='bold')
    axes[1, 0].grid(True, alpha=0.3)
    
    # Learning Rate (if available)
    if 'lr' in history:
        axes[1, 1].plot(history['lr'], label='Learning Rate', linewidth=2, color='orange')
        axes[1, 1].axvline(x=phase_separator, color='red', linestyle='--', alpha=0.7, label='Fine-tune start')
        axes[1, 1].set_title('Learning Rate Schedule', fontweight='bold')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Learning Rate')
        axes[1, 1].set_yscale('log')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
    else:
        axes[1, 1].axis('off')
    
    # Loss vs Accuracy correlation
    axes[1, 2].scatter(history['loss'], history['accuracy'], alpha=0.6, label='Training', s=20)
    axes[1, 2].scatter(history['val_loss'], history['val_accuracy'], alpha=0.6, label='Validation', s=20)
    axes[1, 2].set_title('Loss vs Accuracy Correlation', fontweight='bold')
    axes[1, 2].set_xlabel('Loss')
    axes[1, 2].set_ylabel('Accuracy')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.suptitle(f'{Config.MODEL_NAME} - Comprehensive Training Analysis', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    plot_path = os.path.join(results_dir, f"{Config.MODEL_NAME}_training_history.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"‚úÖ Training plots saved to: {plot_path}")

def main():
    """Main function"""
    print("ü©∫ DenseNet121 Transfer Learning 4-Class Training")
    print("=" * 50)
    
    # Setup directories
    output_dir = config.output.get_output_path(Config.MODEL_NAME)
    model_save_dir = config.output.get_output_path(Config.MODEL_NAME, config.output.models_subdir)
    results_dir = config.output.get_output_path(Config.MODEL_NAME, config.output.results_subdir)
    
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(model_save_dir, exist_ok=True)
    os.makedirs(results_dir, exist_ok=True)
    
    # Check dataset
    if not os.path.exists(Config.DATASET_PATH):
        print(f"‚ùå Dataset not found: {Config.DATASET_PATH}")
        return
    
    # GPU setup
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"üöÄ Using GPU: {len(gpus)} device(s)")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è GPU setup error: {e}")
    else:
        print("üíª Using CPU")
    
    # Create data generators
    train_gen, val_gen, test_gen = create_data_generators()
    
    # Create model
    model = create_densenet121_transfer_model(fine_tune=False)
    
    print("üìä Model Summary:")
    print(f"üìà Total parameters: {model.count_params():,}")
    print(f"üîí Trainable parameters: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}")
    
    # Two-phase training
    model, combined_history = train_model_two_phase(model, train_gen, val_gen, model_save_dir)
    
    # Plot training history
    plot_training_history(combined_history, results_dir)
    
    # Load best model for evaluation
    best_model_path = os.path.join(model_save_dir, f"best_{Config.MODEL_NAME}_phase2.h5")
    if os.path.exists(best_model_path):
        print("üìÇ Loading best model for evaluation...")
        model = keras.models.load_model(best_model_path)
    
    # Evaluate model
    results = evaluate_model(model, test_gen, results_dir)
    
    # Save final model
    final_model_path = os.path.join(model_save_dir, f"final_{Config.MODEL_NAME}.h5")
    model.save(final_model_path)
    
    print(f"\nüéâ Training completed!")
    print(f"üìÅ Results saved in: {output_dir}")
    print(f"üéØ Test Accuracy: {results['test_accuracy']:.4f}")
    print(f"üèÜ Best class F1-score: {max([results['per_class_metrics'][cls]['f1-score'] for cls in Config.CLASS_NAMES]):.4f}")
    print("=" * 50)

if __name__ == "__main__":
    main()